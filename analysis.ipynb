{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking metrics for clearing and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Metrics (from papers):\n",
      "Actual amount of foreign capital used in the current year\n",
      "Actual amount of foreign investment used in the year\n",
      "Actual amount of foreign investment utilized\n",
      "Actual utilization of foreign direct investment amount\n",
      "Added value of the secondary industry as a proportion of GDP\n",
      "Added value of the tertiary industry as a proportion of GDP\n",
      "Administrative area land area\n",
      "Advanced industrial structure index\n",
      "Advanced level of industrial structure\n",
      "Amount of foreign capital utilized\n",
      "Amount of foreign capital utilized in the current year\n",
      "Annual growth rate of fixed asset investment\n",
      "Annual output value of tertiary industry\n",
      "Annual percentage growth rate of GDP\n",
      "Average wage of employees\n",
      "Average wage of on-the-job workers\n",
      "Balance of savings deposits per capita\n",
      "Degree of resource depletion\n",
      "Dependence on foreign trade\n",
      "Dependence on foreign trade (Total imports and exports/GDP)\n",
      "Deposit balance of financial institutions\n",
      "Development intensity of construction land per 10,000 GDP\n",
      "Digital industrialization (Internet industry employment index)\n",
      "Disposal income of urban residents\n",
      "Economic innovation ability\n",
      "Education expenditure\n",
      "Expenditure on education\n",
      "Expenditure on science and technology\n",
      "Financial interrelations ratio\n",
      "Financial revenue\n",
      "Financial self-reliance rate\n",
      "Fiscal deficit rate\n",
      "Fiscal deficit ratio\n",
      "Fiscal revenue per capita\n",
      "Fiscal self-sufficiency rate\n",
      "Fixed asset investment\n",
      "Fixed asset investment per capita\n",
      "Fixed assets investment\n",
      "Fixed assets investment as a percentage of GDP\n",
      "Foreign capital dependence\n",
      "Foreign direct investment contract projects\n",
      "Foreign trade dependence\n",
      "GDP\n",
      "GDP per capita\n",
      "General industrial solid waste comprehensive utilization rate\n",
      "General public budget revenue\n",
      "Green area per capita\n",
      "Green coverage rate in built-up areas\n",
      "Gross domestic product per capita\n",
      "Gross tourism receipts\n",
      "Harmless disposal rate of household garbage\n",
      "Harmless treatment rate of domestic garbage\n",
      "Highway freight volume\n",
      "Highway passenger volume\n",
      "Household registration population at the end of the year\n",
      "Household saving deposits at year-end\n",
      "Increment in the ratio of investment in fixed assets and GDP\n",
      "Industrial SO2 emissions per 10,000 GDP\n",
      "Industrial dust emissions per 10,000 GDP\n",
      "Industrial enterprises above the designated size\n",
      "Industrial structure concentration\n",
      "Industrial structure diversification index\n",
      "Industrial structure upgrading\n",
      "Industrial wastewater discharge per 10,000 GDP\n",
      "Innovation index\n",
      "International Internet households per 100 population\n",
      "Internet broadband access users\n",
      "Internet penetration\n",
      "Investment in fixed assets as a percentage of GDP\n",
      "Investment in urban public facilities construction\n",
      "Judicial protection of intellectual property rights\n",
      "Labor productivity (GDP/Number of employees in urban units)\n",
      "Loan-to-deposit ratio\n",
      "Local public finance expenditure\n",
      "Mobile Internet penetration\n",
      "Mobile phone year-end users\n",
      "Mobile phones per 100 population\n",
      "Natural growth rate\n",
      "Number of banking institutions per 10,000 persons\n",
      "Number of colleges and universities\n",
      "Number of doctors\n",
      "Number of effective invention patents per 10,000 people\n",
      "Number of employees in the tertiary industry\n",
      "Number of employees in urban units at the end of the period\n",
      "Number of enterprises above designated size\n",
      "Number of industrial enterprises above the scale\n",
      "Number of patents granted\n",
      "Number of patents granted per 10,000 persons\n",
      "Number of patents per 10,000 people\n",
      "Number of people with health insurance per 10,000 people\n",
      "Number of projects for contracted foreign investment\n",
      "Number of scientific research and technical service employees\n",
      "Number of students enrolled in regular colleges per 10,000 persons\n",
      "Number of students in higher education per 10,000 people\n",
      "Overall energy consumption per 10,000 GDP\n",
      "Per capita GDP\n",
      "Per capita deposits in savings at end of year\n",
      "Per capita disposable income\n",
      "Per capita disposable income of urban residents\n",
      "Per capita fiscal budget income\n",
      "Per capita fiscal expenditure on education\n",
      "Per capita fiscal expenditure on science\n",
      "Per capita fiscal revenue\n",
      "Per capita fixed asset investment\n",
      "Per capita fixed-asset investment\n",
      "Per capita general public budget revenue\n",
      "Per capita gross regional product\n",
      "Per capita imports and exports\n",
      "Per capita investment in fixed assets\n",
      "Per capita local fiscal expenditure\n",
      "Per capita regional financial spending\n",
      "Per capita retail sales amount of consumer goods\n",
      "Per capita retail sales of social consumer goods\n",
      "Per capita retail sales of social consumption\n",
      "Per capita sales of retail social consumer goods\n",
      "Per capita savings balance\n",
      "Per capita total retail sales of consumer goods\n",
      "Per capita year-end balance of urban and rural resident savings\n",
      "Per household savings deposits at year-end\n",
      "Per public finance income\n",
      "Population density\n",
      "Postal business revenue\n",
      "Prices stability\n",
      "Primary industry as a percentage of GDP\n",
      "Primary sector as a proportion of GDP\n",
      "Proportion of GDP in the second and third industries\n",
      "Proportion of GDP increased by the tertiary industry\n",
      "Proportion of R&D investment in GDP\n",
      "Proportion of employees of state-owned and collective-owned units\n",
      "Proportion of expenditure for education, science, and technology\n",
      "Proportion of financial expenditure on science and technology\n",
      "Proportion of fiscal revenue in GDP\n",
      "Proportion of persons employed in resource industries\n",
      "Proportion of persons employed in scientific research and technical services\n",
      "Proportion of private and individual employment in urban areas\n",
      "Proportion of science and education expenditures to fiscal expenses\n",
      "Proportion of science and education in fiscal expenditure\n",
      "Proportion of science and technology expenditure in fiscal expenditure\n",
      "Proportion of tertiary industries in GDP\n",
      "Proportion of tertiary industry\n",
      "Proportion of tertiary industry in GDP\n",
      "Proportion of the population with a minimum living allowance\n",
      "Proportion of the secondary industry added value in GDP\n",
      "Proportion of the tertiary industry added value in GDP\n",
      "Proportion of total import and export\n",
      "Public budget expenditure\n",
      "Rate for consolidated utilization of industrial solid waste\n",
      "Rate of non-hazardous treatment of living waste\n",
      "Rate of reuse of industrial wastewater\n",
      "Ratio of R&D expenditure to GDP\n",
      "Ratio of deposits and loans to financial institutions\n",
      "Ratio of direct economic losses due to disasters to regional GDP\n",
      "Ratio of expenditure for research and experimental development to fiscal expenditure\n",
      "Ratio of gross domestic fixed investment to GDP\n",
      "Ratio of intramural expenditure on R&D and GDP\n",
      "Ratio of investment in fixed assets and GDP\n",
      "Ratio of large and medium-sized enterprises to small enterprises\n",
      "Ratio of liabilities and assets of enterprises above a designated level\n",
      "Ratio of local fiscal revenue to GDP\n",
      "Ratio of public finance income and expenditure\n",
      "Ratio of tertiary value added to GDP\n",
      "Ratio of the actual use of foreign capital to GDP\n",
      "Ratio of the added value of high-tech industries to industrial added value\n",
      "Ratio of the added value of the tertiary industry to GDP\n",
      "Ratio of the output value of secondary and tertiary industries\n",
      "Regional innovation and entrepreneurship index\n",
      "Registered unemployment rate\n",
      "Registered urban unemployment rate\n",
      "Residentsâ€™ savings deposit balance\n",
      "Retail sales of consumer goods per capita\n",
      "Savings balance per capita\n",
      "Savings deposit balance per capita\n",
      "Science and technology expenditure\n",
      "Scientific expenditure as a percentage of fiscal expenditure\n",
      "Scientific research industry employment index\n",
      "Sewage treatment plant centralized treatment rate\n",
      "Share of unemployed population in urban areas\n",
      "Social insurance coverage rate\n",
      "Software business revenue\n",
      "Support policies\n",
      "Tertiary industry as a percentage of GDP\n",
      "Tertiary sector as a percentage of GDP\n",
      "The tertiary industryâ€™s share of GDP\n",
      "The third industry accounts for the proportion of GDP\n",
      "Three major patent licenses\n",
      "Total employment\n",
      "Total export-import per capita\n",
      "Total exportâ€“import volume\n",
      "Total import and export trade as a percentage of GDP\n",
      "Total import and export volume\n",
      "Total number of Chinese and English papers\n",
      "Total population\n",
      "Total retail sales of consumer goods\n",
      "Total retail sales of social consumer goods\n",
      "Total retail sales of social consumer goods as a percentage of GDP\n",
      "Unemployment rate\n",
      "Urban construction land\n",
      "Urban disposable income per capita\n",
      "Urban per capita GDP\n",
      "Urban per capita disposable income of residents\n",
      "Urbanization rate\n",
      "\n",
      "Metrics not in the YAML:\n",
      "Actual amount of foreign capital used in the current year\n",
      "Actual amount of foreign investment used in the year\n",
      "Actual amount of foreign investment utilized\n",
      "Actual utilization of foreign direct investment amount\n",
      "Added value of the secondary industry as a proportion of GDP\n",
      "Added value of the tertiary industry as a proportion of GDP\n",
      "Administrative area land area\n",
      "Advanced industrial structure index\n",
      "Advanced level of industrial structure\n",
      "Amount of foreign capital utilized\n",
      "Amount of foreign capital utilized in the current year\n",
      "Annual growth rate of fixed asset investment\n",
      "Annual output value of tertiary industry\n",
      "Annual percentage growth rate of GDP\n",
      "Average wage of employees\n",
      "Average wage of on-the-job workers\n",
      "Balance of savings deposits per capita\n",
      "Degree of resource depletion\n",
      "Dependence on foreign trade\n",
      "Dependence on foreign trade (Total imports and exports/GDP)\n",
      "Deposit balance of financial institutions\n",
      "Development intensity of construction land per 10,000 GDP\n",
      "Digital industrialization (Internet industry employment index)\n",
      "Disposal income of urban residents\n",
      "Economic innovation ability\n",
      "Education expenditure\n",
      "Expenditure on education\n",
      "Expenditure on science and technology\n",
      "Financial interrelations ratio\n",
      "Financial revenue\n",
      "Financial self-reliance rate\n",
      "Fiscal deficit rate\n",
      "Fiscal deficit ratio\n",
      "Fiscal revenue per capita\n",
      "Fiscal self-sufficiency rate\n",
      "Fixed asset investment\n",
      "Fixed asset investment per capita\n",
      "Fixed assets investment\n",
      "Fixed assets investment as a percentage of GDP\n",
      "Foreign capital dependence\n",
      "Foreign direct investment contract projects\n",
      "Foreign trade dependence\n",
      "GDP\n",
      "GDP per capita\n",
      "General industrial solid waste comprehensive utilization rate\n",
      "General public budget revenue\n",
      "Green area per capita\n",
      "Green coverage rate in built-up areas\n",
      "Gross domestic product per capita\n",
      "Gross tourism receipts\n",
      "Harmless disposal rate of household garbage\n",
      "Harmless treatment rate of domestic garbage\n",
      "Highway freight volume\n",
      "Highway passenger volume\n",
      "Household registration population at the end of the year\n",
      "Household saving deposits at year-end\n",
      "Increment in the ratio of investment in fixed assets and GDP\n",
      "Industrial SO2 emissions per 10,000 GDP\n",
      "Industrial dust emissions per 10,000 GDP\n",
      "Industrial enterprises above the designated size\n",
      "Industrial structure concentration\n",
      "Industrial structure diversification index\n",
      "Industrial structure upgrading\n",
      "Industrial wastewater discharge per 10,000 GDP\n",
      "Innovation index\n",
      "International Internet households per 100 population\n",
      "Internet broadband access users\n",
      "Internet penetration\n",
      "Investment in fixed assets as a percentage of GDP\n",
      "Investment in urban public facilities construction\n",
      "Judicial protection of intellectual property rights\n",
      "Labor productivity (GDP/Number of employees in urban units)\n",
      "Loan-to-deposit ratio\n",
      "Local public finance expenditure\n",
      "Mobile Internet penetration\n",
      "Mobile phone year-end users\n",
      "Mobile phones per 100 population\n",
      "Natural growth rate\n",
      "Number of banking institutions per 10,000 persons\n",
      "Number of colleges and universities\n",
      "Number of doctors\n",
      "Number of effective invention patents per 10,000 people\n",
      "Number of employees in the tertiary industry\n",
      "Number of employees in urban units at the end of the period\n",
      "Number of enterprises above designated size\n",
      "Number of industrial enterprises above the scale\n",
      "Number of patents granted\n",
      "Number of patents granted per 10,000 persons\n",
      "Number of patents per 10,000 people\n",
      "Number of people with health insurance per 10,000 people\n",
      "Number of projects for contracted foreign investment\n",
      "Number of scientific research and technical service employees\n",
      "Number of students enrolled in regular colleges per 10,000 persons\n",
      "Number of students in higher education per 10,000 people\n",
      "Overall energy consumption per 10,000 GDP\n",
      "Per capita GDP\n",
      "Per capita deposits in savings at end of year\n",
      "Per capita disposable income\n",
      "Per capita disposable income of urban residents\n",
      "Per capita fiscal budget income\n",
      "Per capita fiscal expenditure on education\n",
      "Per capita fiscal expenditure on science\n",
      "Per capita fiscal revenue\n",
      "Per capita fixed asset investment\n",
      "Per capita fixed-asset investment\n",
      "Per capita general public budget revenue\n",
      "Per capita gross regional product\n",
      "Per capita imports and exports\n",
      "Per capita investment in fixed assets\n",
      "Per capita local fiscal expenditure\n",
      "Per capita regional financial spending\n",
      "Per capita retail sales amount of consumer goods\n",
      "Per capita retail sales of social consumer goods\n",
      "Per capita retail sales of social consumption\n",
      "Per capita sales of retail social consumer goods\n",
      "Per capita savings balance\n",
      "Per capita total retail sales of consumer goods\n",
      "Per capita year-end balance of urban and rural resident savings\n",
      "Per household savings deposits at year-end\n",
      "Per public finance income\n",
      "Population density\n",
      "Postal business revenue\n",
      "Prices stability\n",
      "Primary industry as a percentage of GDP\n",
      "Primary sector as a proportion of GDP\n",
      "Proportion of GDP in the second and third industries\n",
      "Proportion of GDP increased by the tertiary industry\n",
      "Proportion of R&D investment in GDP\n",
      "Proportion of employees of state-owned and collective-owned units\n",
      "Proportion of expenditure for education, science, and technology\n",
      "Proportion of financial expenditure on science and technology\n",
      "Proportion of fiscal revenue in GDP\n",
      "Proportion of persons employed in resource industries\n",
      "Proportion of persons employed in scientific research and technical services\n",
      "Proportion of private and individual employment in urban areas\n",
      "Proportion of science and education expenditures to fiscal expenses\n",
      "Proportion of science and education in fiscal expenditure\n",
      "Proportion of science and technology expenditure in fiscal expenditure\n",
      "Proportion of tertiary industries in GDP\n",
      "Proportion of tertiary industry\n",
      "Proportion of tertiary industry in GDP\n",
      "Proportion of the population with a minimum living allowance\n",
      "Proportion of the secondary industry added value in GDP\n",
      "Proportion of the tertiary industry added value in GDP\n",
      "Proportion of total import and export\n",
      "Public budget expenditure\n",
      "Rate for consolidated utilization of industrial solid waste\n",
      "Rate of non-hazardous treatment of living waste\n",
      "Rate of reuse of industrial wastewater\n",
      "Ratio of R&D expenditure to GDP\n",
      "Ratio of deposits and loans to financial institutions\n",
      "Ratio of direct economic losses due to disasters to regional GDP\n",
      "Ratio of expenditure for research and experimental development to fiscal expenditure\n",
      "Ratio of gross domestic fixed investment to GDP\n",
      "Ratio of intramural expenditure on R&D and GDP\n",
      "Ratio of investment in fixed assets and GDP\n",
      "Ratio of large and medium-sized enterprises to small enterprises\n",
      "Ratio of liabilities and assets of enterprises above a designated level\n",
      "Ratio of local fiscal revenue to GDP\n",
      "Ratio of public finance income and expenditure\n",
      "Ratio of tertiary value added to GDP\n",
      "Ratio of the actual use of foreign capital to GDP\n",
      "Ratio of the added value of high-tech industries to industrial added value\n",
      "Ratio of the added value of the tertiary industry to GDP\n",
      "Ratio of the output value of secondary and tertiary industries\n",
      "Regional innovation and entrepreneurship index\n",
      "Registered unemployment rate\n",
      "Registered urban unemployment rate\n",
      "Residentsâ€™ savings deposit balance\n",
      "Retail sales of consumer goods per capita\n",
      "Savings balance per capita\n",
      "Savings deposit balance per capita\n",
      "Science and technology expenditure\n",
      "Scientific expenditure as a percentage of fiscal expenditure\n",
      "Scientific research industry employment index\n",
      "Sewage treatment plant centralized treatment rate\n",
      "Share of unemployed population in urban areas\n",
      "Social insurance coverage rate\n",
      "Software business revenue\n",
      "Support policies\n",
      "Tertiary industry as a percentage of GDP\n",
      "Tertiary sector as a percentage of GDP\n",
      "The tertiary industryâ€™s share of GDP\n",
      "The third industry accounts for the proportion of GDP\n",
      "Three major patent licenses\n",
      "Total employment\n",
      "Total export-import per capita\n",
      "Total exportâ€“import volume\n",
      "Total import and export trade as a percentage of GDP\n",
      "Total import and export volume\n",
      "Total number of Chinese and English papers\n",
      "Total population\n",
      "Total retail sales of consumer goods\n",
      "Total retail sales of social consumer goods\n",
      "Total retail sales of social consumer goods as a percentage of GDP\n",
      "Unemployment rate\n",
      "Urban construction land\n",
      "Urban disposable income per capita\n",
      "Urban per capita GDP\n",
      "Urban per capita disposable income of residents\n",
      "Urbanization rate\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "def load_yaml(file_path):\n",
    "    \"\"\"Load a YAML file and return its content.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "def extract_names_and_synonyms(data, key):\n",
    "    \"\"\"\n",
    "    Extract names and synonyms from a clustered YAML structure.\n",
    "    \n",
    "    Args:\n",
    "        data (dict): Data structure containing clustered data.\n",
    "        key (str): Key for the cluster, e.g., 'clustered_metrics' or 'clustered_methods'.\n",
    "    \n",
    "    Returns:\n",
    "        set: A set of all names and synonyms.\n",
    "    \"\"\"\n",
    "    items = set()\n",
    "    for cluster in data.get(key, []):\n",
    "        for item in cluster.get(key[:-1], []):  # dynamically access 'metric' or 'method'\n",
    "            items.add(item.get('name'))\n",
    "            items.update(item.get('synonyms', []))\n",
    "    return items\n",
    "\n",
    "def extract_items_from_papers(data, item_type):\n",
    "    \"\"\"\n",
    "    Extract all unique items (metrics or methods) from the 'papers' section.\n",
    "    \n",
    "    Args:\n",
    "        data (dict): Data structure containing the 'papers' section.\n",
    "        item_type (str): 'metrics' or 'methods'.\n",
    "    \n",
    "    Returns:\n",
    "        set: A set of unique items found in the papers.\n",
    "    \"\"\"\n",
    "    return {item for paper in data.get('papers', []) for item in paper.get(item_type, [])}\n",
    "\n",
    "def main():\n",
    "    metrics_yaml_path = \"dictionaries/metrics.yml\"\n",
    "    papers_yaml_path = \"papers.yml\"\n",
    "    \n",
    "    # Load YAML data\n",
    "    metrics_data = load_yaml(metrics_yaml_path)\n",
    "    papers_data = load_yaml(papers_yaml_path)\n",
    "    \n",
    "    # Extract metrics\n",
    "    extracted_metrics = extract_items_from_papers(papers_data, 'metrics')\n",
    "    yaml_metrics = extract_names_and_synonyms(metrics_data, 'clustered_metrics')\n",
    "    \n",
    "    # Find missing metrics\n",
    "    missing_metrics = extracted_metrics - yaml_metrics\n",
    "    \n",
    "    # Output results\n",
    "    print(\"All Metrics (from papers):\")\n",
    "    print(\"\\n\".join(sorted(extracted_metrics)))\n",
    "    \n",
    "    print(\"\\nMetrics not in the YAML:\")\n",
    "    print(\"\\n\".join(sorted(missing_metrics)))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking methods for clearing and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Methods (from papers):\n",
      "Adaptive cycle theory\n",
      "ArcGIS for resilience visualization\n",
      "Bartlett's test of sphericity\n",
      "CRITIC and EWM combined weighting model\n",
      "CRITICâ€“TOPSIS weighted evaluation model\n",
      "Coefficient of variation method (CV)\n",
      "Combination weight method\n",
      "Combined weight model\n",
      "Complex network approach\n",
      "Coupling coordination degree (CCD) analysis\n",
      "Coupling coordination degree (CCD) model\n",
      "Coupling coordination degree model\n",
      "Coupling coordination model\n",
      "Dagum's Gini coefficient\n",
      "Driving-Pressure-State-Response (DPSR) model\n",
      "Entropy method\n",
      "Entropy method for three-level indicator weighting\n",
      "Entropy method for urban resilience index calculation\n",
      "Entropy value method\n",
      "Entropy weight method\n",
      "Entropy weight method (EWM)\n",
      "Entropy weight method (Shannon, 1948)\n",
      "Entropy weight method with time series weights\n",
      "Entropy weight-TOPSIS method\n",
      "Evaluation functions for persistence, adaptability, and transformation\n",
      "Exploratory Spatial Data Analysis (ESDA)\n",
      "Extreme value entropy method\n",
      "Fuzzy logic reasoning method\n",
      "Geographical detector model\n",
      "Geographically weighted regression (GWR)\n",
      "Global entropy weighting method\n",
      "Grey correlation analysis\n",
      "Instrumental variable estimation methods\n",
      "Intuitionistic fuzzy set theory\n",
      "Intuitionistic trapezoidal fuzzy number\n",
      "Kaiser-Meyer-Olkin (KMO) test\n",
      "Kernel Density Estimation (KDE)\n",
      "Kernel density estimation\n",
      "Kernel density estimation method\n",
      "Maximizing deviation method\n",
      "Mediating effect model\n",
      "Meta-frontier global super-efficiency SBM model with undesired outputs\n",
      "Moderated mediation model\n",
      "Modified coupled coordination model\n",
      "Moran's I spatial correlation analysis\n",
      "Multi-index weighted summation method\n",
      "Multilevel urban resilience evaluation index system\n",
      "Obstacle degree analysis\n",
      "Obstacle degree model\n",
      "Particle swarm optimization (PSO)-back propagation (BP) neural network\n",
      "Principal Component Analysis (PCA)\n",
      "Revised coupling coordination method\n",
      "Social opportunity function\n",
      "Spatial econometrics\n",
      "Spatial measurement model\n",
      "Spatial regression models (SLM and SEM)\n",
      "Subjective and objective weighting method\n",
      "TOPSIS method\n",
      "Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS)\n",
      "Theil index\n",
      "Time-varying Difference-in-Differences (DID) model\n",
      "Two-way fixed effect model\n",
      "Urban resilience capacity index\n",
      "Urban resilience evaluation model\n",
      "\n",
      "Methods not in the YAML:\n",
      "Adaptive cycle theory\n",
      "ArcGIS for resilience visualization\n",
      "Bartlett's test of sphericity\n",
      "CRITIC and EWM combined weighting model\n",
      "CRITICâ€“TOPSIS weighted evaluation model\n",
      "Coefficient of variation method (CV)\n",
      "Combination weight method\n",
      "Combined weight model\n",
      "Complex network approach\n",
      "Coupling coordination degree (CCD) analysis\n",
      "Coupling coordination degree (CCD) model\n",
      "Coupling coordination degree model\n",
      "Coupling coordination model\n",
      "Dagum's Gini coefficient\n",
      "Driving-Pressure-State-Response (DPSR) model\n",
      "Entropy method\n",
      "Entropy method for three-level indicator weighting\n",
      "Entropy method for urban resilience index calculation\n",
      "Entropy value method\n",
      "Entropy weight method\n",
      "Entropy weight method (EWM)\n",
      "Entropy weight method (Shannon, 1948)\n",
      "Entropy weight method with time series weights\n",
      "Entropy weight-TOPSIS method\n",
      "Evaluation functions for persistence, adaptability, and transformation\n",
      "Exploratory Spatial Data Analysis (ESDA)\n",
      "Extreme value entropy method\n",
      "Fuzzy logic reasoning method\n",
      "Geographical detector model\n",
      "Geographically weighted regression (GWR)\n",
      "Global entropy weighting method\n",
      "Grey correlation analysis\n",
      "Instrumental variable estimation methods\n",
      "Intuitionistic fuzzy set theory\n",
      "Intuitionistic trapezoidal fuzzy number\n",
      "Kaiser-Meyer-Olkin (KMO) test\n",
      "Kernel Density Estimation (KDE)\n",
      "Kernel density estimation\n",
      "Kernel density estimation method\n",
      "Maximizing deviation method\n",
      "Mediating effect model\n",
      "Meta-frontier global super-efficiency SBM model with undesired outputs\n",
      "Moderated mediation model\n",
      "Modified coupled coordination model\n",
      "Moran's I spatial correlation analysis\n",
      "Multi-index weighted summation method\n",
      "Multilevel urban resilience evaluation index system\n",
      "Obstacle degree analysis\n",
      "Obstacle degree model\n",
      "Particle swarm optimization (PSO)-back propagation (BP) neural network\n",
      "Principal Component Analysis (PCA)\n",
      "Revised coupling coordination method\n",
      "Social opportunity function\n",
      "Spatial econometrics\n",
      "Spatial measurement model\n",
      "Spatial regression models (SLM and SEM)\n",
      "Subjective and objective weighting method\n",
      "TOPSIS method\n",
      "Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS)\n",
      "Theil index\n",
      "Time-varying Difference-in-Differences (DID) model\n",
      "Two-way fixed effect model\n",
      "Urban resilience capacity index\n",
      "Urban resilience evaluation model\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "def load_yaml(file_path):\n",
    "    \"\"\"Load YAML content from a file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "def extract_names_and_synonyms(data, key):\n",
    "    \"\"\"\n",
    "    Extract names and synonyms from a clustered YAML structure.\n",
    "    \n",
    "    Args:\n",
    "        data (dict): Data structure containing clustered data.\n",
    "        key (str): Key for the cluster, e.g., 'clustered_metrics' or 'clustered_methods'.\n",
    "    \n",
    "    Returns:\n",
    "        set: A set of all names and synonyms.\n",
    "    \"\"\"\n",
    "    items = set()\n",
    "    for cluster in data.get(key, []):\n",
    "        for item in cluster.get(key[:-1], []):  # dynamically access 'metric' or 'method'\n",
    "            items.add(item.get('name'))\n",
    "            items.update(item.get('synonyms', []))\n",
    "    return items\n",
    "\n",
    "def extract_items_from_papers(data, item_type):\n",
    "    \"\"\"\n",
    "    Extract all unique items (metrics or methods) from the 'papers' section.\n",
    "    \n",
    "    Args:\n",
    "        data (dict): Data structure containing the 'papers' section.\n",
    "        item_type (str): 'metrics' or 'methods'.\n",
    "    \n",
    "    Returns:\n",
    "        set: A set of unique items found in the papers.\n",
    "    \"\"\"\n",
    "    return {item for paper in data.get('papers', []) for item in paper.get(item_type, [])}\n",
    "\n",
    "def main():\n",
    "    methods_yaml_path = \"dictionaries/methods.yml\"\n",
    "    papers_yaml_path = \"papers.yml\"\n",
    "    \n",
    "    # Load YAML data\n",
    "    methods_data = load_yaml(methods_yaml_path)\n",
    "    papers_data = load_yaml(papers_yaml_path)\n",
    "    \n",
    "    # Extract methods\n",
    "    extracted_methods = extract_items_from_papers(papers_data, 'methods')\n",
    "    yaml_methods = extract_names_and_synonyms(methods_data, 'clustered_methods')\n",
    "    \n",
    "    # Find missing methods\n",
    "    missing_methods = extracted_methods - yaml_methods\n",
    "    \n",
    "    # Output results\n",
    "    print(\"All Methods (from papers):\")\n",
    "    print(\"\\n\".join(sorted(extracted_methods)))\n",
    "    \n",
    "    print(\"\\nMethods not in the YAML:\")\n",
    "    print(\"\\n\".join(sorted(missing_methods)))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear and cluster papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated papers saved to cleared_papers.yml\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import yaml\n",
    "\n",
    "# --- Utility Functions --- #\n",
    "def load_yaml(file_path):\n",
    "    \"\"\"Load YAML data from a file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "def save_yaml(data, file_path):\n",
    "    \"\"\"Save YAML data to a file.\"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        yaml.dump(data, file, allow_unicode=True, sort_keys=False)\n",
    "\n",
    "# --- Map Items to Categories --- #\n",
    "def map_items_to_categories(cluster_data, items_list, item_key):\n",
    "    \"\"\"\n",
    "    Map items to their categories based on cluster data.\n",
    "\n",
    "    Args:\n",
    "        cluster_data (list): Cluster definitions with categories.\n",
    "        items_list (list): List of items to categorize.\n",
    "        item_key (str): Key in the cluster data ('metrics' or 'methods').\n",
    "\n",
    "    Returns:\n",
    "        dict: Categorized items as {category: [items]}.\n",
    "    \"\"\"\n",
    "    categorized_items = defaultdict(list)\n",
    "\n",
    "    for item in items_list:\n",
    "        matched = False  # Track if the item is matched to a category\n",
    "        for cluster in cluster_data:\n",
    "            category = cluster['category']\n",
    "            for definition in cluster[item_key]:\n",
    "                # Normalize strings to lowercase for comparison\n",
    "                item_normalized = item.lower()\n",
    "                name_normalized = definition['name'].lower()\n",
    "                synonyms_normalized = [syn.lower() for syn in definition.get('synonyms', [])]\n",
    "\n",
    "                # Match item against the name or synonyms\n",
    "                if item_normalized == name_normalized or item_normalized in synonyms_normalized:\n",
    "                    categorized_items[category].append(definition['name'])  # Use the primary name\n",
    "                    matched = True\n",
    "                    break\n",
    "            if matched:\n",
    "                break\n",
    "        if not matched:\n",
    "            categorized_items[\"Uncategorized\"].append(item)\n",
    "\n",
    "    return categorized_items\n",
    "\n",
    "# --- Process Papers --- #\n",
    "def process_papers(clustered_metrics_path, clustered_methods_path, papers_path, output_path):\n",
    "    \"\"\"\n",
    "    Process papers to categorize metrics and methods.\n",
    "\n",
    "    Args:\n",
    "        clustered_metrics_path (str): Path to clustered metrics YAML.\n",
    "        clustered_methods_path (str): Path to clustered methods YAML.\n",
    "        papers_path (str): Path to papers YAML.\n",
    "        output_path (str): Path to save updated papers YAML.\n",
    "    \"\"\"\n",
    "    # Load cluster data\n",
    "    metrics_data = load_yaml(clustered_metrics_path)\n",
    "    methods_data = load_yaml(clustered_methods_path)\n",
    "    papers_data = load_yaml(papers_path)\n",
    "\n",
    "    clustered_metrics = metrics_data.get('clustered_metrics', [])\n",
    "    clustered_methods = methods_data.get('clustered_methods', [])\n",
    "\n",
    "    if not clustered_metrics or not clustered_methods:\n",
    "        raise ValueError(\"Clustered metrics or methods data is missing in YAML files.\")\n",
    "\n",
    "    # Process each paper\n",
    "    for paper in papers_data.get('papers', []):\n",
    "        for key, cluster_data in ((\"metrics\", clustered_metrics), (\"methods\", clustered_methods)):\n",
    "            if key in paper:\n",
    "                categorized = map_items_to_categories(cluster_data, paper[key], key)\n",
    "                # Replace the original list with the categorized structure\n",
    "                paper[key] = [{'category': cat, 'items': items} for cat, items in categorized.items()]\n",
    "\n",
    "    # Save updated papers\n",
    "    save_yaml(papers_data, output_path)\n",
    "    print(f\"Updated papers saved to {output_path}\")\n",
    "\n",
    "# --- Main --- #\n",
    "def main():\n",
    "    clustered_metrics_path = \"dictionaries/metrics.yml\"\n",
    "    clustered_methods_path = \"dictionaries/methods.yml\"\n",
    "    papers_path = \"papers.yml\"\n",
    "    output_path = \"cleared_papers.yml\"\n",
    "\n",
    "    process_papers(clustered_metrics_path, clustered_methods_path, papers_path, output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PAPER ANALYSIS ===\n",
      "Total papers: 22\n",
      "Unique methods: 45\n",
      "Unique metrics: 124\n",
      "\n",
      "Top 10 Methods:\n",
      "  Entropy weight method: 12\n",
      "  Coupling coordination model: 6\n",
      "  Kernel density estimation: 3\n",
      "  Obstacle degree model: 2\n",
      "  Exploratory Spatial Data Analysis (ESDA): 2\n",
      "  Principal Component Analysis (PCA): 2\n",
      "  Combined weight method: 2\n",
      "  TOPSIS method: 2\n",
      "  Urban resilience evaluation model: 1\n",
      "  Urban resilience capacity index: 1\n",
      "\n",
      "Top 10 Metrics:\n",
      "  GDP per capita: 22\n",
      "  Proportion of the tertiary industry added value in GDP: 12\n",
      "  Fixed asset investment: 10\n",
      "  Urban per capita disposable income of residents: 8\n",
      "  Per capita deposits in savings: 8\n",
      "  Actual amount of foreign capital used: 7\n",
      "  Per capita retail sales of consumer goods: 7\n",
      "  Financial revenue: 6\n",
      "  Dependence on foreign trade: 6\n",
      "  Education expenditure: 5\n",
      "\n",
      "Creating visualizations...\n",
      "\n",
      "Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "\n",
    "# Ensure plots directory exists\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "def load_papers(file_path):\n",
    "    \"\"\"Load papers from YAML file.\"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    return data[\"papers\"]\n",
    "\n",
    "def count_methods_and_metrics(papers):\n",
    "    \"\"\"Count all methods and metrics from papers.\"\"\"\n",
    "    methods = Counter()\n",
    "    metrics = Counter()\n",
    "    method_categories = Counter()\n",
    "    metric_categories = Counter()\n",
    "    \n",
    "    for paper in papers:\n",
    "        # Count methods\n",
    "        for method in paper.get(\"methods\", []):\n",
    "            method_categories[method[\"category\"]] += 1\n",
    "            methods.update(method[\"items\"])\n",
    "        \n",
    "        # Count metrics\n",
    "        for metric in paper.get(\"metrics\", []):\n",
    "            metric_categories[metric[\"category\"]] += 1\n",
    "            metrics.update(metric[\"items\"])\n",
    "    \n",
    "    return methods, metrics, method_categories, metric_categories\n",
    "\n",
    "def get_cooccurrences(papers, item_type=\"methods\"):\n",
    "    \"\"\"Find which categories appear together in papers.\"\"\"\n",
    "    cooccurrences = Counter()\n",
    "    \n",
    "    for paper in papers:\n",
    "        categories = [item[\"category\"] for item in paper.get(item_type, [])]\n",
    "        # Get all unique pairs\n",
    "        for pair in combinations(sorted(set(categories)), 2):\n",
    "            cooccurrences[pair] += 1\n",
    "    \n",
    "    return cooccurrences\n",
    "\n",
    "def create_cooccurrence_matrix(cooccurrences, all_categories):\n",
    "    \"\"\"Create matrix from cooccurrence pairs.\"\"\"\n",
    "    labels = sorted(all_categories.keys())\n",
    "    n = len(labels)\n",
    "    matrix = np.zeros((n, n))\n",
    "    \n",
    "    # Map labels to indices\n",
    "    label_to_idx = {label: i for i, label in enumerate(labels)}\n",
    "    \n",
    "    # Fill matrix\n",
    "    for (cat1, cat2), count in cooccurrences.items():\n",
    "        if cat1 in label_to_idx and cat2 in label_to_idx:\n",
    "            i, j = label_to_idx[cat1], label_to_idx[cat2]\n",
    "            matrix[i, j] = count\n",
    "            matrix[j, i] = count  # Make symmetric\n",
    "    \n",
    "    return matrix, labels\n",
    "\n",
    "def plot_bar_chart(items, title, figsize=(16, 6), save_path=None):\n",
    "    \"\"\"Create bar chart with blue gradient colors.\"\"\"\n",
    "    if not items:\n",
    "        return\n",
    "    \n",
    "    # Sort alphabetically by label\n",
    "    sorted_items = sorted(items, key=lambda x: x[0])\n",
    "    labels, counts = zip(*sorted_items)\n",
    "    colors = plt.cm.Blues(np.linspace(0.9, 0.4, len(labels)))\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    bars = plt.bar(labels, counts, color=colors)\n",
    "    \n",
    "    # Add value labels on bars - ensure integer values\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, height + 0.1,\n",
    "                f'{int(height)}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylim(0, max(counts) * 1.15)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def plot_heatmap(matrix, labels, title, figsize=(16, 8), save_path=None):\n",
    "    \"\"\"Create heatmap with blue colors and annotations.\"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    max_val = int(np.max(matrix))\n",
    "    \n",
    "    # Custom blue colormap\n",
    "    blues_colors = plt.cm.Blues(np.linspace(0, 0.9, 256))\n",
    "    cmap = mcolors.ListedColormap(blues_colors)\n",
    "    \n",
    "    # Create heatmap without colorbar\n",
    "    im = plt.imshow(matrix, cmap=cmap, interpolation='nearest')\n",
    "    \n",
    "    # Set labels\n",
    "    plt.xticks(range(len(labels)), labels, rotation=45, ha='right', fontsize=10)\n",
    "    plt.yticks(range(len(labels)), labels, fontsize=10)\n",
    "    plt.title(title, fontsize=14)\n",
    "    \n",
    "    # Add text annotations - ensure integer values\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            if i != j and matrix[i, j] > 0:  # Skip diagonal and zeros\n",
    "                # Choose text color based on background\n",
    "                text_color = 'white' if matrix[i, j] > max_val * 0.6 else 'black'\n",
    "                plt.text(j, i, f'{int(matrix[i, j])}', \n",
    "                        ha='center', va='center', fontsize=9, \n",
    "                        color=text_color)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def print_stats(papers, methods, metrics):\n",
    "    \"\"\"Print basic statistics.\"\"\"\n",
    "    print(\"=== PAPER ANALYSIS ===\")\n",
    "    print(f\"Total papers: {len(papers)}\")\n",
    "    print(f\"Unique methods: {len(methods)}\")\n",
    "    print(f\"Unique metrics: {len(metrics)}\")\n",
    "    \n",
    "    print(f\"\\nTop 10 Methods:\")\n",
    "    for method, count in methods.most_common(10):\n",
    "        print(f\"  {method}: {count}\")\n",
    "    \n",
    "    print(f\"\\nTop 10 Metrics:\")\n",
    "    for metric, count in metrics.most_common(10):\n",
    "        print(f\"  {metric}: {count}\")\n",
    "\n",
    "def analyze_papers():\n",
    "    \"\"\"Main analysis function.\"\"\"\n",
    "    # Load data\n",
    "    papers = load_papers(\"cleared_papers.yml\")\n",
    "    \n",
    "    # Count everything\n",
    "    methods, metrics, method_cats, metric_cats = count_methods_and_metrics(papers)\n",
    "    \n",
    "    # Print statistics\n",
    "    print_stats(papers, methods, metrics)\n",
    "    \n",
    "    # Create visualizations\n",
    "    print(\"\\nCreating visualizations...\")\n",
    "    \n",
    "    # Bar charts\n",
    "    plot_bar_chart(methods.most_common(10), \"Common Methods\",\n",
    "               save_path=\"plots/common_methods.png\")\n",
    "    plot_bar_chart(metrics.most_common(10), \"Common Metrics\",\n",
    "                save_path=\"plots/common_metrics.png\")\n",
    "    plot_bar_chart(list(metric_cats.items()), \"Metric Categories\",\n",
    "                save_path=\"plots/metric_categories.png\")\n",
    "    \n",
    "    # Cooccurrence analysis\n",
    "    method_cooc = get_cooccurrences(papers, \"methods\")\n",
    "    metric_cooc = get_cooccurrences(papers, \"metrics\")\n",
    "    \n",
    "    # Heatmaps\n",
    "    if method_cooc:\n",
    "        matrix, labels = create_cooccurrence_matrix(method_cooc, method_cats)\n",
    "        plot_heatmap(matrix, labels, \"Method Category Co-occurrence\",\n",
    "                    save_path=\"plots/method_cooccurrence.png\")\n",
    "\n",
    "    if metric_cooc:\n",
    "        matrix, labels = create_cooccurrence_matrix(metric_cooc, metric_cats)\n",
    "        plot_heatmap(matrix, labels, \"Metric Category Co-occurrence\",\n",
    "                    save_path=\"plots/metric_cooccurrence.png\")\n",
    "    \n",
    "    print(\"\\nAnalysis complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_papers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import pandas as pd\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "def load_papers(file_path):\n",
    "    \"\"\"Load papers from YAML file.\"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    return data[\"papers\"]\n",
    "\n",
    "def get_years_data(papers):\n",
    "    \"\"\"Extract all years from papers' temporal scopes.\"\"\"\n",
    "    years = []\n",
    "    for paper in papers:\n",
    "        years.extend(paper[\"temporal_scope\"])\n",
    "    return np.array(years)\n",
    "\n",
    "def create_timeline_plot(years, save_path=None):\n",
    "    \"\"\"Create gradient violin plot showing timeline distribution.\"\"\"\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    # Calculate density\n",
    "    kde = gaussian_kde(years)\n",
    "    min_year = years.min()\n",
    "    max_year = years.max()\n",
    "    x_points = np.linspace(min_year, max_year, 300)\n",
    "    density = kde(x_points)\n",
    "    \n",
    "    # Scale density for plotting\n",
    "    max_width = 0.4\n",
    "    scaled_density = density / density.max() * max_width\n",
    "    \n",
    "    # Create custom blues colormap\n",
    "    blues_slice = mcolors.ListedColormap(plt.cm.Blues(np.linspace(0.4, 0.9, 256)))\n",
    "    \n",
    "    # Draw gradient-filled violin\n",
    "    for i in range(len(x_points) - 1):\n",
    "        x_coords = [x_points[i], x_points[i+1], x_points[i+1], x_points[i]]\n",
    "        y_coords = [-scaled_density[i], -scaled_density[i+1], \n",
    "                    scaled_density[i+1], scaled_density[i]]\n",
    "        \n",
    "        color_value = (density[i] + density[i+1]) / (2 * density.max())\n",
    "        color = blues_slice(color_value)\n",
    "        \n",
    "        ax.fill(x_coords, y_coords, color=color, edgecolor=color)\n",
    "    \n",
    "    # Add center line\n",
    "    ax.plot([min_year, max_year], [0, 0], color='black', linewidth=1)\n",
    "    \n",
    "    # Format plot\n",
    "    ax.set_title(\"Timeline of Papers' Temporal Coverage\", fontsize=16)\n",
    "    ax.set_xlabel(\"Year\", fontsize=12)\n",
    "    ax.set_ylabel(\"Density\", fontsize=12)\n",
    "    ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
    "    ax.set_xticks(np.arange(min_year, max_year + 1, 1))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def main():\n",
    "    papers = load_papers(\"cleared_papers.yml\")\n",
    "    years = get_years_data(papers)\n",
    "    create_timeline_plot(years, save_path=\"plots/timeline.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/__/wpdz8q855f1_1xrzt7zv5w200000gn/T/ipykernel_10829/2980167857.py:60: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import textwrap\n",
    "\n",
    "def load_papers(file_path):\n",
    "    \"\"\"Load papers from YAML file.\"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    return data.get(\"papers\", [])\n",
    "\n",
    "def get_metrics_data(papers):\n",
    "    \"\"\"Extract metrics and years from papers.\"\"\"\n",
    "    data = []\n",
    "    for paper in papers:\n",
    "        year = paper.get(\"year\")\n",
    "        if year:\n",
    "            for metric in paper.get(\"metrics\", []):\n",
    "                for item in metric.get(\"items\", []):\n",
    "                    data.append({\"Metric\": item, \"Year\": year})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def prepare_data(df, top_n=10):\n",
    "    \"\"\"Get top metrics and calculate percentages.\"\"\"\n",
    "    top_metrics = df[\"Metric\"].value_counts().head(top_n).index\n",
    "    df_filtered = df[df[\"Metric\"].isin(top_metrics)]\n",
    "    \n",
    "    counts = df_filtered.groupby([\"Metric\", \"Year\"]).size().reset_index(name=\"Count\")\n",
    "    year_totals = df.groupby(\"Year\").size().reset_index(name=\"Year_Total\")\n",
    "    result = counts.merge(year_totals, on=\"Year\")\n",
    "    result[\"Percentage\"] = (result[\"Count\"] / result[\"Year_Total\"] * 100).round(1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def create_plot(data, save_path=None):\n",
    "    \"\"\"Create scatter plot with seaborn, optionally save to file.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(16, 7))\n",
    "    blues_slice = mcolors.ListedColormap(plt.cm.Blues(np.linspace(0.4, 0.9, 256)))\n",
    "    \n",
    "    scatter = sns.scatterplot(\n",
    "        data=data,\n",
    "        x=\"Metric\", \n",
    "        y=\"Year\", \n",
    "        size=\"Percentage\",\n",
    "        sizes=(100, 2500),\n",
    "        hue=\"Percentage\", \n",
    "        palette=blues_slice,\n",
    "        alpha=0.8,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=0.5,\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    # Wrap long labels\n",
    "    labels = [\"\\n\".join(textwrap.wrap(label, 15)) for label in data[\"Metric\"].unique()]\n",
    "    ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "    \n",
    "    # Move legend to the right side\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(\n",
    "        handles, labels,\n",
    "        title=\"Frequency (%)\",\n",
    "        loc=\"center left\",\n",
    "        bbox_to_anchor=(1.02, 0.5),\n",
    "        frameon=False,\n",
    "        borderpad=2,\n",
    "        labelspacing=4,\n",
    "        handletextpad=2,\n",
    "        borderaxespad=0.5\n",
    "    )\n",
    "    \n",
    "    # Format plot\n",
    "    ax.set_title(\"Distribution of Common Metrics Over Time (Normalized by Year)\")\n",
    "    ax.set_xlabel(\"Metrics\")\n",
    "    ax.set_ylabel(\"Year\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def main():\n",
    "    papers = load_papers(\"cleared_papers.yml\")\n",
    "    df = get_metrics_data(papers)\n",
    "    plot_data = prepare_data(df)\n",
    "    create_plot(plot_data, save_path=\"plots/metrics_over_time.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import Normalize\n",
    "from community import community_louvain\n",
    "import yaml\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "def load_papers(filename):\n",
    "    \"\"\"Load papers from a YAML file.\"\"\"\n",
    "    with open(filename, \"r\") as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    return data[\"papers\"]\n",
    "\n",
    "def create_bipartite_graph(papers):\n",
    "    \"\"\"Create a bipartite graph of papers and methods.\"\"\"\n",
    "    edges = []\n",
    "    for paper in papers:\n",
    "        paper_id = paper[\"id\"]\n",
    "        methods = [method for category in paper.get(\"methods\", []) for method in category[\"items\"]]\n",
    "        for method in methods:\n",
    "            edges.append((paper_id, method))\n",
    "    B = nx.Graph()\n",
    "    B.add_edges_from(edges)\n",
    "    return B\n",
    "\n",
    "def project_graph(B):\n",
    "    \"\"\"Project the bipartite graph into a unipartite graph of papers.\"\"\"\n",
    "    paper_nodes = [node for node in B.nodes if isinstance(node, int)]\n",
    "    return nx.bipartite.projected_graph(B, paper_nodes)\n",
    "\n",
    "def visualize_graph(papers_graph, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize the paper graph with community detection and node coloring.\n",
    "    If save_path is provided, save the figure (PNG, 300 dpi) and close it.\n",
    "    Otherwise, show interactively.\n",
    "    \"\"\"\n",
    "    if papers_graph.number_of_nodes() == 0:\n",
    "        print(\"Graph is empty; nothing to plot.\")\n",
    "        return\n",
    "\n",
    "    # Community partition (unused for color here, but kept for potential extensions)\n",
    "    _communities = community_louvain.best_partition(papers_graph)\n",
    "\n",
    "    # Layout and node metrics\n",
    "    pos = nx.spring_layout(papers_graph, seed=42, k=5, iterations=300)\n",
    "    degrees = [papers_graph.degree(node) for node in papers_graph.nodes]\n",
    "\n",
    "    # Color map by degree\n",
    "    norm = Normalize(vmin=min(degrees), vmax=max(degrees))\n",
    "    blues_slice = mcolors.ListedColormap(plt.cm.Blues(np.linspace(0.4, 0.9, 256)))\n",
    "    node_colors = [blues_slice(norm(d)) for d in degrees]\n",
    "    node_sizes = [2000 + 100 * d for d in degrees]\n",
    "\n",
    "    # Draw\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    nx.draw_networkx_edges(papers_graph, pos, alpha=0.3, edge_color=\"gray\")\n",
    "    nx.draw_networkx_nodes(papers_graph, pos, node_size=node_sizes, node_color=node_colors, alpha=0.9)\n",
    "\n",
    "    labels = {n: f\"Paper {n}\" if isinstance(n, int) else n for n in papers_graph.nodes}\n",
    "    nx.draw_networkx_labels(papers_graph, pos, labels, font_size=8, font_color=\"white\")\n",
    "\n",
    "    # Colorbar for degree\n",
    "    fig = plt.gcf()\n",
    "    sm = ScalarMappable(cmap=blues_slice, norm=norm)\n",
    "    sm.set_array([])\n",
    "    cbar = fig.colorbar(sm, ax=plt.gca(), orientation=\"vertical\")\n",
    "    cbar.set_label(\"Node Degree\")\n",
    "    cbar.ax.yaxis.set_major_formatter(FormatStrFormatter('%d'))\n",
    "\n",
    "    plt.title(\"Network of Papers Based on Shared Methods\", fontsize=16)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the analysis and plotting.\"\"\"\n",
    "    papers = load_papers(\"cleared_papers.yml\")\n",
    "    B = create_bipartite_graph(papers)\n",
    "    papers_graph = project_graph(B)\n",
    "    visualize_graph(papers_graph, save_path=\"plots/paper_network.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 metric categories\n",
      "Creating network for: Economic & Investment\n",
      "Saved: plots/paper_network_economic__investment.png\n",
      "  - 22 papers, 210 connections\n",
      "Creating network for: Environment & Resources\n",
      "Saved: plots/paper_network_environment__resources.png\n",
      "  - 4 papers, 1 connections\n",
      "Creating network for: Finance & Revenue\n",
      "Saved: plots/paper_network_finance__revenue.png\n",
      "  - 20 papers, 42 connections\n",
      "Creating network for: Industrial & Structural\n",
      "Saved: plots/paper_network_industrial__structural.png\n",
      "  - 20 papers, 72 connections\n",
      "Creating network for: Miscellaneous & Others\n",
      "Saved: plots/paper_network_miscellaneous__others.png\n",
      "  - 7 papers, 0 connections\n",
      "Creating network for: Population & Employment\n",
      "Saved: plots/paper_network_population__employment.png\n",
      "  - 10 papers, 4 connections\n",
      "Creating network for: Retail & Consumption\n",
      "Saved: plots/paper_network_retail__consumption.png\n",
      "  - 11 papers, 22 connections\n",
      "Creating network for: Social & Living Standards\n",
      "Saved: plots/paper_network_social__living_standards.png\n",
      "  - 16 papers, 40 connections\n",
      "Creating network for: Technology & Innovation\n",
      "Saved: plots/paper_network_technology__innovation.png\n",
      "  - 10 papers, 6 connections\n",
      "Creating network for: Urban & Infrastructure\n",
      "Saved: plots/paper_network_urban__infrastructure.png\n",
      "  - 7 papers, 1 connections\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import yaml\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "def load_papers(file_path):\n",
    "    \"\"\"Load papers from YAML file.\"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    return data.get(\"papers\", [])\n",
    "\n",
    "def extract_categories(papers):\n",
    "    \"\"\"Extract all unique metric categories.\"\"\"\n",
    "    categories = set()\n",
    "    for paper in papers:\n",
    "        for metric in paper.get(\"metrics\", []):\n",
    "            categories.add(metric[\"category\"])\n",
    "    return sorted(categories)\n",
    "\n",
    "def create_paper_network(papers, category):\n",
    "    \"\"\"Create network of papers connected by shared metrics.\"\"\"\n",
    "    edges = []\n",
    "    for paper in papers:\n",
    "        paper_id = paper[\"id\"]\n",
    "        for metric in paper.get(\"metrics\", []):\n",
    "            if metric[\"category\"] == category:\n",
    "                for item in metric[\"items\"]:\n",
    "                    edges.append((paper_id, item))\n",
    "    if not edges:\n",
    "        return None\n",
    "\n",
    "    # Create bipartite graph and project to paper-only network\n",
    "    bipartite_graph = nx.Graph()\n",
    "    bipartite_graph.add_edges_from(edges)\n",
    "\n",
    "    paper_nodes = [node for node in bipartite_graph.nodes if isinstance(node, int)]\n",
    "    return nx.bipartite.projected_graph(bipartite_graph, paper_nodes)\n",
    "\n",
    "def calculate_smart_ticks(max_value, max_ticks=8):\n",
    "    \"\"\"Generate intelligent tick locations for colorbar.\"\"\"\n",
    "    if max_value <= max_ticks:\n",
    "        return list(range(max_value + 1))\n",
    "    if max_value <= 20:\n",
    "        step = 2 if max_value <= 16 else 5\n",
    "    elif max_value <= 50:\n",
    "        step = 5 if max_value <= 30 else 10\n",
    "    else:\n",
    "        step = max(1, max_value // max_ticks)\n",
    "    ticks = list(range(0, max_value + 1, step))\n",
    "    if ticks[-1] != max_value:\n",
    "        ticks.append(max_value)\n",
    "    return ticks\n",
    "\n",
    "def setup_plot_elements(graph):\n",
    "    \"\"\"Calculate positions, colors, and sizes for network visualization.\"\"\"\n",
    "    pos = nx.spring_layout(graph, seed=42, k=5, iterations=300)\n",
    "    degrees = [graph.degree(node) for node in graph.nodes]\n",
    "    max_degree = max(degrees) if degrees else 0\n",
    "\n",
    "    vmin, vmax = (0, 1) if max_degree == 0 else (0, max_degree)\n",
    "    norm = Normalize(vmin=vmin, vmax=vmax)\n",
    "    blues_slice = mcolors.ListedColormap(plt.cm.Blues(np.linspace(0.4, 0.9, 256)))\n",
    "\n",
    "    return {\n",
    "        \"pos\": pos,\n",
    "        \"degrees\": degrees,\n",
    "        \"max_degree\": max_degree,\n",
    "        \"norm\": norm,\n",
    "        \"colormap\": blues_slice,\n",
    "        \"node_colors\": [blues_slice(norm(d)) for d in degrees],\n",
    "        \"node_sizes\": [2000 + 100 * d for d in degrees],\n",
    "    }\n",
    "\n",
    "def _slugify(text):\n",
    "    \"\"\"Filesystem-friendly slug for filenames.\"\"\"\n",
    "    text = re.sub(r\"\\s+\", \"_\", text.strip())\n",
    "    text = re.sub(r\"[^\\w\\-_.]\", \"\", text)\n",
    "    return text.lower() or \"category\"\n",
    "\n",
    "def create_network_plot(graph, category, save_dir=\"plots\", show=False):\n",
    "    \"\"\"Create network visualization; save PNG to save_dir and optionally show.\"\"\"\n",
    "    if not graph or len(graph.nodes) == 0:\n",
    "        print(f\"No network found for category: {category}\")\n",
    "        return\n",
    "\n",
    "    elements = setup_plot_elements(graph)\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    # Draw network\n",
    "    nx.draw_networkx_edges(graph, elements[\"pos\"], alpha=0.3, edge_color=\"gray\", ax=ax)\n",
    "    nx.draw_networkx_nodes(\n",
    "        graph,\n",
    "        elements[\"pos\"],\n",
    "        node_size=elements[\"node_sizes\"],\n",
    "        node_color=elements[\"node_colors\"],\n",
    "        alpha=0.9,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    # Labels\n",
    "    labels = {node: f\"Paper {node}\" for node in graph.nodes}\n",
    "    nx.draw_networkx_labels(\n",
    "        graph, elements[\"pos\"], labels, font_size=8, font_color=\"white\", ax=ax\n",
    "    )\n",
    "\n",
    "    # Colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap=elements[\"colormap\"], norm=elements[\"norm\"])\n",
    "    sm.set_array([])\n",
    "    cbar = fig.colorbar(sm, ax=ax)\n",
    "    cbar.set_label(\"Node Degree\")\n",
    "\n",
    "    # Smart ticks\n",
    "    tick_locations = (\n",
    "        [0, 1] if elements[\"max_degree\"] == 0 else calculate_smart_ticks(elements[\"max_degree\"])\n",
    "    )\n",
    "    cbar.set_ticks(tick_locations)\n",
    "    cbar.set_ticklabels([str(int(t)) for t in tick_locations])\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_title(f\"Paper Network: {category}\", fontsize=14)\n",
    "    ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save to file\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    fname = f\"paper_network_{_slugify(category)}.png\"\n",
    "    out_path = os.path.join(save_dir, fname)\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    print(f\"Saved: {out_path}\")\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"  - {len(graph.nodes)} papers, {len(graph.edges)} connections\")\n",
    "\n",
    "def analyze_networks(show=False):\n",
    "    \"\"\"Main analysis function.\"\"\"\n",
    "    papers = load_papers(\"cleared_papers.yml\")\n",
    "    categories = extract_categories(papers)\n",
    "    print(f\"Found {len(categories)} metric categories\")\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Creating network for: {category}\")\n",
    "        paper_graph = create_paper_network(papers, category)\n",
    "        create_network_plot(paper_graph, category, save_dir=\"plots\", show=show)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    analyze_networks(show=False)  # set True to also display\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ID                                                                                             Author                                                                                          Method Category                                                                                                                                                                                                                               Metric Category    Timespan\n",
      "  1                                                                    Na Chen, Hongxia Guo, Hui Xiang                                              Weighting Models; Theory-based Methods; Index-based Methods                                                                                                                   Social & Living Standards; Population & Employment; Economic & Investment; Industrial & Structural; Technology & Innovation 2010 - 2020\n",
      "  2                                                                   Haiyue Fu, Nana Hong, Chuan Liao                             Index-based Methods; Spatial Analysis Tools; Regression and Mediation Models                                                                                                                            Economic & Investment; Finance & Revenue; Industrial & Structural; Retail & Consumption; Social & Living Standards        2020\n",
      "  3                                                Yuanyuan Ge, Wenjuan Jia, Hui Zhao, Pengcheng Xiang                                      Weighting Models; Index-based Methods; Coupling Coordination Models                                                                                                                            Industrial & Structural; Finance & Revenue; Economic & Investment; Retail & Consumption; Social & Living Standards 2012 - 2021\n",
      "  4                                             Shan Han, Bo Wang, Yibin Ao, Homa Bahmani, Beibei Chai                                                           Weighting Models; Coupling Coordination Models                                                                                                                            Economic & Investment; Industrial & Structural; Finance & Revenue; Social & Living Standards; Retail & Consumption 2005 - 2020\n",
      "  5                                 Songtao He, Shuigen Yang, Amar Razzaq, Sahar Erfanian, Azhar Abbas                                                                          Regression and Mediation Models                          Economic & Investment; Social & Living Standards; Miscellaneous & Others; Population & Employment; Retail & Consumption; Finance & Revenue; Technology & Innovation; Industrial & Structural; Urban & Infrastructure 2004 - 2017\n",
      "  6                                                   Yingzi Lin, Chong Peng, Peng Chen, Mengjie Zhang                                                           Weighting Models; Coupling Coordination Models                                                                                                Economic & Investment; Finance & Revenue; Industrial & Structural; Technology & Innovation; Population & Employment; Social & Living Standards 2008 - 2017\n",
      "  7                                                              Hao Lu, Xin Lu, Liudan Jiao, Yu Zhang            Network and Optimization Models; Weighting Models; Density Estimation; Spatial Analysis Tools                                                                                                                            Economic & Investment; Social & Living Standards; Finance & Revenue; Retail & Consumption; Industrial & Structural 2015 - 2019\n",
      "  8                                                                             Ruocan Lu, Zhihui Yang                                Weighting Models; Network and Optimization Models; Spatial Analysis Tools                                                                     Economic & Investment; Population & Employment; Retail & Consumption; Urban & Infrastructure; Environment & Resources; Technology & Innovation; Social & Living Standards 2013 - 2020\n",
      "  9             Fei Ma, Zuohang Wang, Qipeng Sun, Kum Fai Yuen, Yanxia Zhang, Huifeng Xue, Shumei Zhao                                                       Weighting Models; Distance and Similarity Measures                                                                                                                                                                                                      Economic & Investment; Finance & Revenue 2009 - 2016\n",
      " 10                                                                                    Xin Ma, Fen Jia         Weighting Models; Density Estimation; Inequality and Efficiency Measures; Spatial Analysis Tools Miscellaneous & Others; Industrial & Structural; Social & Living Standards; Economic & Investment; Population & Employment; Finance & Revenue; Environment & Resources; Urban & Infrastructure; Retail & Consumption; Technology & Innovation 2006 - 2021\n",
      " 11                                            Juntao Tan, Pingyu Zhang, Kevin Lo, Jing Li, Shiwei Liu                                                                  Statistical Tests; Theory-based Methods                                               Industrial & Structural; Finance & Revenue; Environment & Resources; Social & Living Standards; Economic & Investment; Population & Employment; Miscellaneous & Others; Technology & Innovation 2003 - 2013\n",
      " 12                                                                              Bo Tang, Zechuang Tan                                                                    Weighting Models; Index-based Methods                                                                                                                                                  Economic & Investment; Social & Living Standards; Industrial & Structural; Finance & Revenue 2010 - 2020\n",
      " 13                            Decai Tang, Jiannan Li, Ziqian Zhao, Valentina Boamah, David D. Lansana                                                                    Weighting Models; Visualization Tools                                                                                                                                                       Economic & Investment; Finance & Revenue; Industrial & Structural; Retail & Consumption 2010 - 2019\n",
      " 14                                                                                 Jia Wang, Xia Zhou Theory-based Methods; Weighting Models; Inequality and Efficiency Measures; Coupling Coordination Models                                                     Urban & Infrastructure; Population & Employment; Finance & Revenue; Economic & Investment; Industrial & Structural; Retail & Consumption; Technology & Innovation; Miscellaneous & Others 2003 - 2021\n",
      " 15                                                              Ke-Liang Wang, Wei Jiang, Zhuang Miao                                                        Weighting Models; Regression and Mediation Models                                                                                                                                                                             Economic & Investment; Finance & Revenue; Industrial & Structural 2003 - 2019\n",
      " 16                                    Xin Xu, Meimei Wang, Mingfeng Wang, Yongchun Yang, Yuliang Wang                                                        Fuzzy-based Methods; Coupling Coordination Models                                                                                                 Economic & Investment; Industrial & Structural; Social & Living Standards; Population & Employment; Urban & Infrastructure; Finance & Revenue        2019\n",
      " 17                                                                           Xiaolin Xun, Yongbo Yuan                                  Fuzzy-based Methods; Distance and Similarity Measures; Weighting Models                                                                                                                         Economic & Investment; Industrial & Structural; Finance & Revenue; Social & Living Standards; Technology & Innovation 2013 - 2017\n",
      " 18                                                                            Tingting Yang, Lin Wang                                                                                         Weighting Models                                                                         Social & Living Standards; Economic & Investment; Finance & Revenue; Urban & Infrastructure; Industrial & Structural; Technology & Innovation; Miscellaneous & Others 2005 - 2021\n",
      " 19                                                       Haichao Yu, Yan Liu, Chengliang Liu, Fei Fan                    Weighting Models; Distance and Similarity Measures; Density Estimation; Miscellaneous                       Population & Employment; Industrial & Structural; Miscellaneous & Others; Urban & Infrastructure; Economic & Investment; Finance & Revenue; Social & Living Standards; Technology & Innovation; Environment & Resources 2004 - 2016\n",
      " 20                                                     Yihua Yu, Caili Yang, Qingsha Hu, Shuning Kong                                                       Statistical Tests; Regression and Mediation Models                                                                                                                                                     Economic & Investment; Finance & Revenue; Industrial & Structural; Miscellaneous & Others 2002 - 2018\n",
      " 21 Maomao Zhang, Weigang Chen, Kui Cai, Xin Gao, Xuesong Zhang, Jinxiang Liu, Zhiyuan Wang, Deshou Li                                                                 Weighting Models; Spatial Analysis Tools                                                                                                   Finance & Revenue; Population & Employment; Industrial & Structural; Social & Living Standards; Economic & Investment; Retail & Consumption 2006 - 2017\n",
      " 22                                                                              Ying Zhang, Yunyan Li                                      Weighting Models; Coupling Coordination Models; Index-based Methods                                                                                                                                                       Economic & Investment; Retail & Consumption; Industrial & Structural; Finance & Revenue 2010 - 2021\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "\n",
    "def load_yaml(filename):\n",
    "    \"\"\"Load the YAML file and return the list of papers.\"\"\"\n",
    "    with open(filename, \"r\") as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    return data.get(\"papers\", [])\n",
    "\n",
    "def extract_method_categories(methods):\n",
    "    \"\"\"\n",
    "    Extract method categories from the list of methods.\n",
    "    Returns a semicolon-separated string of method categories.\n",
    "    \"\"\"\n",
    "    categories = [method.get(\"category\", \"\") for method in methods if \"category\" in method]\n",
    "    return \"; \".join(categories)\n",
    "\n",
    "def extract_metric_categories(metrics):\n",
    "    \"\"\"\n",
    "    Extract metric categories from the list of metrics.\n",
    "    Returns a semicolon-separated string of metric categories.\n",
    "    \"\"\"\n",
    "    categories = [metric.get(\"category\", \"\") for metric in metrics if \"category\" in metric]\n",
    "    return \"; \".join(categories)\n",
    "\n",
    "def extract_timespan(temporal_scope):\n",
    "    \"\"\"\n",
    "    Given a list of years (temporal_scope), return a string showing the start and end.\n",
    "    If no temporal scope is provided, return \"N/A\".\n",
    "    \"\"\"\n",
    "    if temporal_scope and len(temporal_scope) >= 2:\n",
    "        start = temporal_scope[0]\n",
    "        end = temporal_scope[-1]\n",
    "        return f\"{start} - {end}\"\n",
    "    elif temporal_scope:\n",
    "        # If only one year is present, show that year.\n",
    "        return str(temporal_scope[0])\n",
    "    else:\n",
    "        return \"N/A\"\n",
    "\n",
    "def build_review_table(papers):\n",
    "    \"\"\"\n",
    "    Build a pandas DataFrame suited for a systematic literature review.\n",
    "    Columns: ID, Author, Method Category, Metric Category, Timespan.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for paper in papers:\n",
    "        paper_id = paper.get(\"id\", \"\")\n",
    "        authors = \", \".join(paper.get(\"authors\", []))\n",
    "        method_categories = extract_method_categories(paper.get(\"methods\", []))\n",
    "        metric_categories = extract_metric_categories(paper.get(\"metrics\", []))\n",
    "        timespan = extract_timespan(paper.get(\"temporal_scope\", []))\n",
    "        \n",
    "        rows.append({\n",
    "            \"ID\": paper_id,\n",
    "            \"Author\": authors,\n",
    "            \"Method Category\": method_categories,\n",
    "            \"Metric Category\": metric_categories,\n",
    "            \"Timespan\": timespan\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def main():\n",
    "    # Load the papers from the YAML file.\n",
    "    papers_file = \"cleared_papers.yml\"\n",
    "    papers = load_yaml(papers_file)\n",
    "    \n",
    "    # Build the review table.\n",
    "    review_table = build_review_table(papers)\n",
    "    \n",
    "    # Print the table as plain text.\n",
    "    print(review_table.to_string(index=False))\n",
    "    \n",
    "    # Optionally, export the table to a CSV file:\n",
    "    # review_table.to_csv(\"review_table.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
